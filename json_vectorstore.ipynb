{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13615d11",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     43\u001b[39m     documents.append(Document(page_content=text.strip(), metadata=data))\n\u001b[32m     45\u001b[39m llm = ChatOpenAI(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0\u001b[39m)\n\u001b[32m     46\u001b[39m db = Chroma.from_documents(\n\u001b[32m     47\u001b[39m     documents=[\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m         Document(page_content=doc.page_content, metadata=\u001b[43mfilter_complex_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     49\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents\n\u001b[32m     50\u001b[39m     ],\n\u001b[32m     51\u001b[39m     embedding=embedding_function\n\u001b[32m     52\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\utils.py:66\u001b[39m, in \u001b[36mfilter_complex_metadata\u001b[39m\u001b[34m(documents, allowed_types)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[32m     65\u001b[39m     filtered_metadata = {}\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdocument\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m.items():\n\u001b[32m     67\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, allowed_types):\n\u001b[32m     68\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'metadata'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "import json\n",
    "    \n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=r\"C:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\backend\\data\\atc2.json\",\n",
    "    jq_schema=\".ATC_Codes[]\",\n",
    "    text_content=False  # Important: this avoids the ValueError you hit\n",
    ")\n",
    "\n",
    "# Load raw documents\n",
    "raw_docs = loader.load()\n",
    "documents = []\n",
    "\n",
    "\n",
    "\n",
    "for doc in raw_docs:\n",
    "    if isinstance(doc.page_content, str):\n",
    "        data = json.loads(doc.page_content)\n",
    "    elif isinstance(doc.page_content, dict):\n",
    "        data = doc.page_content\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported page_content type\")\n",
    "\n",
    "    text = f\"\"\"\n",
    "    Product-Medikament: {data.get(\"Product-Medikament\")}\n",
    "    Beschreibung: {data.get(\"Beschreibung\")}\n",
    "    Anwendung: {data.get(\"Anwendung\")}\n",
    "    Gruppe: {data.get(\"Gruppe\")}\n",
    "    Hauptkategorie ATC: {data.get(\"ATC Oberkategorie\")}\n",
    "    Unterkategorie ATC: {data.get(\"ATC Unterkategorie\")}\n",
    "    \"\"\"\n",
    "    \n",
    "    documents.append(Document(page_content=text.strip(), metadata=data))\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "db = Chroma.from_documents(\n",
    "    documents=[\n",
    "        Document(page_content=doc.page_content, metadata=filter_complex_metadata(doc.metadata))\n",
    "        for doc in documents\n",
    "    ],\n",
    "    embedding=embedding_function\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "60105171",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m     documents.append(Document(page_content=text.strip(), metadata=safe_metadata))\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Step 5: Create vectorstore\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m db = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mFahRe\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDesktop\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43magentic-LLM-app\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mbackend\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mchroma_db\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:887\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    885\u001b[39m texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    886\u001b[39m metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:843\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[32m    838\u001b[39m         api=chroma_collection._client,\n\u001b[32m    839\u001b[39m         ids=ids,\n\u001b[32m    840\u001b[39m         metadatas=metadatas,\n\u001b[32m    841\u001b[39m         documents=texts,\n\u001b[32m    842\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m         \u001b[43mchroma_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    849\u001b[39m     chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    279\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    280\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    281\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:115\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    113\u001b[39m     sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_kwargs\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings.tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:685\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    682\u001b[39m features.update(extra_features)\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    687\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:758\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    756\u001b[39m     module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m    757\u001b[39m     module_kwargs = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:442\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m trans_features = {\n\u001b[32m    437\u001b[39m     key: value\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    440\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    444\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1144\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1142\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1156\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1157\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    624\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    625\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:253\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:640\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m    639\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:552\u001b[39m, in \u001b[36mBertOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    554\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "\n",
    "# ✅ Utility to clean metadata\n",
    "def clean_metadata(metadata: dict) -> dict:\n",
    "    \"\"\"Ensure all metadata values are Chroma-compatible (str, int, float, bool, None).\"\"\"\n",
    "    allowed_types = (str, int, float, bool, type(None))\n",
    "    cleaned = {}\n",
    "    for k, v in metadata.items():\n",
    "        if isinstance(v, allowed_types):\n",
    "            cleaned[k] = v\n",
    "        elif isinstance(v, list):\n",
    "            cleaned[k] = \", \".join(map(str, v))  # Convert list to CSV string\n",
    "        else:\n",
    "            cleaned[k] = str(v)  # Fallback: convert everything else to string\n",
    "    return cleaned\n",
    "\n",
    "# Step 1: Initialize embedding model\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2: Load JSON\n",
    "loader = JSONLoader(\n",
    "    file_path=r\"C:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\backend\\data\\atc2.json\",\n",
    "    jq_schema=\".ATC_Codes[]\",\n",
    "    text_content=False\n",
    ")\n",
    "\n",
    "# Step 3: Load raw documents\n",
    "raw_docs = loader.load()\n",
    "documents = []\n",
    "\n",
    "# Step 4: Build cleaned Documents\n",
    "for doc in raw_docs:\n",
    "    data = json.loads(doc.page_content) if isinstance(doc.page_content, str) else doc.page_content\n",
    "\n",
    "    # Format the readable content\n",
    "    text = f\"\"\"\n",
    "    Product-Medikament: {data.get(\"Product-Medikament\")}\n",
    "    Beschreibung: {data.get(\"Beschreibung\")}\n",
    "    Anwendung: {data.get(\"Anwendung\")}\n",
    "    Gruppe: {data.get(\"Gruppe\")}\n",
    "    ATC Oberkategorie: {data.get(\"ATC Oberkategorie\")}\n",
    "    ATC Unterkategorie: {data.get(\"ATC Unterkategorie\")}\n",
    "    \"\"\"\n",
    "    safe_metadata = clean_metadata(data)\n",
    "    documents.append(Document(page_content=text.strip(), metadata=safe_metadata))\n",
    "\n",
    "# Step 5: Create vectorstore\n",
    "db = Chroma.from_documents(documents, embedding_function, persist_directory = r\"C:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\backend\\data\\chroma_db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9b64df9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FahRe\\AppData\\Local\\Temp\\ipykernel_45464\\542037117.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top-1 Similar Doc ---\n",
      "Product-Medikament: Ferrodona\n",
      "    Beschreibung: Homöopathisches Arzneimittel bei Blutarmut.\n",
      "    Anwendung: Einnahme vor dem Essen in etwas Wasser.\n",
      "    Gruppe: None\n",
      "    ATC Oberkategorie: B Blut und Blut bildende Organe\n",
      "    ATC Unterkategorie: B03 Antianämika\n",
      "\n",
      "--- Top-2 Similar Doc ---\n",
      "Product-Medikament: Ferrodona\n",
      "    Beschreibung: Homöopathisches Arzneimittel bei Blutarmut.\n",
      "    Anwendung: Einnahme vor dem Essen in etwas Wasser.\n",
      "    Gruppe: None\n",
      "    ATC Oberkategorie: B Blut und Blut bildende Organe\n",
      "    ATC Unterkategorie: B03 Antianämika\n",
      "\n",
      "--- Top-3 Similar Doc ---\n",
      "Product-Medikament: Hibidil Lösung\n",
      "    Beschreibung: Desinfektionsmittel mit dem Wirkstoff Chlorhexidindigluconat, welcher gegen Bakterien, Pilze und andere Erreger wirksam ist. Zur Desinfektion von kleineren Wunden und kleinflächigen leichten Verbrennungen.\n",
      "    Anwendung: None\n",
      "    Gruppe: None\n",
      "    ATC Oberkategorie: D Dermatika\n",
      "    ATC Unterkategorie: D08 Antiseptika und Desinfektionsmittel\n",
      "\n",
      "Antwort: Lactulose\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma(\n",
    "    persist_directory = r\"C:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\backend\\data\\chroma_db\", embedding_function=embedding_function)\n",
    "query = \"Welcher Wirkstoff ist im Duphalac?\"\n",
    "docs = db.similarity_search(query, k=3)\n",
    "retrieved_text = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Optional: print context\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"\\n--- Top-{i+1} Similar Doc ---\\n{d.page_content}\")\n",
    "\n",
    "# Prompt and chain\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    Du bist ein medizinischer Assistent. Nutze den folgenden Kontext, um die Frage so präzise wie möglich zu beantworten.\n",
    "    Wenn die Antwort nicht im Kontext enthalten ist, gib das ehrlich an. Antworte kurz und sachlich.\n",
    "\n",
    "    Kontext: {context}\n",
    "\n",
    "    Frage: {question}\n",
    "    Antwort:\n",
    "    \"\"\"\n",
    ")\n",
    "qa_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "response = qa_chain.run({\"context\": retrieved_text, \"question\": query})\n",
    "\n",
    "print(\"\\nAntwort:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80fde42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 9698, which is longer than the specified 8000\n",
      "Created a chunk of size 15270, which is longer than the specified 8000\n",
      "Created a chunk of size 8758, which is longer than the specified 8000\n",
      "Created a chunk of size 11226, which is longer than the specified 8000\n",
      "Created a chunk of size 8639, which is longer than the specified 8000\n",
      "Created a chunk of size 10245, which is longer than the specified 8000\n",
      "Created a chunk of size 11622, which is longer than the specified 8000\n",
      "Created a chunk of size 12239, which is longer than the specified 8000\n",
      "Created a chunk of size 13803, which is longer than the specified 8000\n",
      "Created a chunk of size 13682, which is longer than the specified 8000\n",
      "Created a chunk of size 11496, which is longer than the specified 8000\n",
      "Created a chunk of size 10869, which is longer than the specified 8000\n",
      "Created a chunk of size 8642, which is longer than the specified 8000\n",
      "Created a chunk of size 13449, which is longer than the specified 8000\n",
      "Created a chunk of size 13033, which is longer than the specified 8000\n",
      "Created a chunk of size 12407, which is longer than the specified 8000\n",
      "Created a chunk of size 22557, which is longer than the specified 8000\n",
      "Created a chunk of size 28778, which is longer than the specified 8000\n",
      "Created a chunk of size 18002, which is longer than the specified 8000\n",
      "Created a chunk of size 11317, which is longer than the specified 8000\n",
      "Created a chunk of size 17515, which is longer than the specified 8000\n",
      "Created a chunk of size 18484, which is longer than the specified 8000\n",
      "Created a chunk of size 8250, which is longer than the specified 8000\n",
      "Created a chunk of size 10405, which is longer than the specified 8000\n",
      "Created a chunk of size 24529, which is longer than the specified 8000\n",
      "Created a chunk of size 17968, which is longer than the specified 8000\n",
      "Created a chunk of size 8506, which is longer than the specified 8000\n",
      "Created a chunk of size 9698, which is longer than the specified 8000\n",
      "Created a chunk of size 15270, which is longer than the specified 8000\n",
      "Created a chunk of size 8758, which is longer than the specified 8000\n",
      "Created a chunk of size 11226, which is longer than the specified 8000\n",
      "Created a chunk of size 8639, which is longer than the specified 8000\n",
      "Created a chunk of size 10245, which is longer than the specified 8000\n",
      "Created a chunk of size 11622, which is longer than the specified 8000\n",
      "Created a chunk of size 12239, which is longer than the specified 8000\n",
      "Created a chunk of size 13803, which is longer than the specified 8000\n",
      "Created a chunk of size 13682, which is longer than the specified 8000\n",
      "Created a chunk of size 11496, which is longer than the specified 8000\n",
      "Created a chunk of size 10869, which is longer than the specified 8000\n",
      "Created a chunk of size 8642, which is longer than the specified 8000\n",
      "Created a chunk of size 13449, which is longer than the specified 8000\n",
      "Created a chunk of size 13033, which is longer than the specified 8000\n",
      "Created a chunk of size 12407, which is longer than the specified 8000\n",
      "Created a chunk of size 22557, which is longer than the specified 8000\n",
      "Created a chunk of size 28778, which is longer than the specified 8000\n",
      "Created a chunk of size 18002, which is longer than the specified 8000\n",
      "Created a chunk of size 11317, which is longer than the specified 8000\n",
      "Created a chunk of size 17515, which is longer than the specified 8000\n",
      "Created a chunk of size 18484, which is longer than the specified 8000\n",
      "Created a chunk of size 8250, which is longer than the specified 8000\n",
      "Created a chunk of size 10405, which is longer than the specified 8000\n",
      "Created a chunk of size 24529, which is longer than the specified 8000\n",
      "Created a chunk of size 17968, which is longer than the specified 8000\n",
      "Created a chunk of size 8506, which is longer than the specified 8000\n",
      "Created a chunk of size 9698, which is longer than the specified 8000\n",
      "Created a chunk of size 15270, which is longer than the specified 8000\n",
      "Created a chunk of size 8758, which is longer than the specified 8000\n",
      "Created a chunk of size 11226, which is longer than the specified 8000\n",
      "Created a chunk of size 8639, which is longer than the specified 8000\n",
      "Created a chunk of size 10245, which is longer than the specified 8000\n",
      "Created a chunk of size 11622, which is longer than the specified 8000\n",
      "Created a chunk of size 12239, which is longer than the specified 8000\n",
      "Created a chunk of size 13803, which is longer than the specified 8000\n",
      "Created a chunk of size 13682, which is longer than the specified 8000\n",
      "Created a chunk of size 11496, which is longer than the specified 8000\n",
      "Created a chunk of size 10869, which is longer than the specified 8000\n",
      "Created a chunk of size 8642, which is longer than the specified 8000\n",
      "Created a chunk of size 13449, which is longer than the specified 8000\n",
      "Created a chunk of size 13033, which is longer than the specified 8000\n",
      "Created a chunk of size 12407, which is longer than the specified 8000\n",
      "Created a chunk of size 22557, which is longer than the specified 8000\n",
      "Created a chunk of size 28778, which is longer than the specified 8000\n",
      "Created a chunk of size 18002, which is longer than the specified 8000\n",
      "Created a chunk of size 11317, which is longer than the specified 8000\n",
      "Created a chunk of size 17515, which is longer than the specified 8000\n",
      "Created a chunk of size 18484, which is longer than the specified 8000\n",
      "Created a chunk of size 8250, which is longer than the specified 8000\n",
      "Created a chunk of size 10405, which is longer than the specified 8000\n",
      "Created a chunk of size 24529, which is longer than the specified 8000\n",
      "Created a chunk of size 17968, which is longer than the specified 8000\n",
      "Created a chunk of size 8506, which is longer than the specified 8000\n",
      "Created a chunk of size 9698, which is longer than the specified 8000\n",
      "Created a chunk of size 15270, which is longer than the specified 8000\n",
      "Created a chunk of size 8758, which is longer than the specified 8000\n",
      "Created a chunk of size 11226, which is longer than the specified 8000\n",
      "Created a chunk of size 8639, which is longer than the specified 8000\n",
      "Created a chunk of size 10245, which is longer than the specified 8000\n",
      "Created a chunk of size 11622, which is longer than the specified 8000\n",
      "Created a chunk of size 12239, which is longer than the specified 8000\n",
      "Created a chunk of size 13803, which is longer than the specified 8000\n",
      "Created a chunk of size 13682, which is longer than the specified 8000\n",
      "Created a chunk of size 11496, which is longer than the specified 8000\n",
      "Created a chunk of size 10869, which is longer than the specified 8000\n",
      "Created a chunk of size 8642, which is longer than the specified 8000\n",
      "Created a chunk of size 13449, which is longer than the specified 8000\n",
      "Created a chunk of size 13033, which is longer than the specified 8000\n",
      "Created a chunk of size 12407, which is longer than the specified 8000\n",
      "Created a chunk of size 22557, which is longer than the specified 8000\n",
      "Created a chunk of size 28778, which is longer than the specified 8000\n",
      "Created a chunk of size 18002, which is longer than the specified 8000\n",
      "Created a chunk of size 11317, which is longer than the specified 8000\n",
      "Created a chunk of size 17515, which is longer than the specified 8000\n",
      "Created a chunk of size 18484, which is longer than the specified 8000\n",
      "Created a chunk of size 8250, which is longer than the specified 8000\n",
      "Created a chunk of size 10405, which is longer than the specified 8000\n",
      "Created a chunk of size 24529, which is longer than the specified 8000\n",
      "Created a chunk of size 17968, which is longer than the specified 8000\n",
      "Created a chunk of size 8506, which is longer than the specified 8000\n",
      "Created a chunk of size 9698, which is longer than the specified 8000\n",
      "Created a chunk of size 15270, which is longer than the specified 8000\n",
      "Created a chunk of size 8758, which is longer than the specified 8000\n",
      "Created a chunk of size 11226, which is longer than the specified 8000\n",
      "Created a chunk of size 8639, which is longer than the specified 8000\n",
      "Created a chunk of size 10245, which is longer than the specified 8000\n",
      "Created a chunk of size 11622, which is longer than the specified 8000\n",
      "Created a chunk of size 12239, which is longer than the specified 8000\n",
      "Created a chunk of size 13803, which is longer than the specified 8000\n",
      "Created a chunk of size 13682, which is longer than the specified 8000\n",
      "Created a chunk of size 11496, which is longer than the specified 8000\n",
      "Created a chunk of size 10869, which is longer than the specified 8000\n",
      "Created a chunk of size 8642, which is longer than the specified 8000\n",
      "Created a chunk of size 13449, which is longer than the specified 8000\n",
      "Created a chunk of size 13033, which is longer than the specified 8000\n",
      "Created a chunk of size 12407, which is longer than the specified 8000\n",
      "Created a chunk of size 22557, which is longer than the specified 8000\n",
      "Created a chunk of size 28778, which is longer than the specified 8000\n",
      "Created a chunk of size 18002, which is longer than the specified 8000\n",
      "Created a chunk of size 11317, which is longer than the specified 8000\n",
      "Created a chunk of size 17515, which is longer than the specified 8000\n",
      "Created a chunk of size 18484, which is longer than the specified 8000\n",
      "Created a chunk of size 8250, which is longer than the specified 8000\n",
      "Created a chunk of size 10405, which is longer than the specified 8000\n",
      "Created a chunk of size 24529, which is longer than the specified 8000\n",
      "Created a chunk of size 17968, which is longer than the specified 8000\n",
      "Created a chunk of size 8506, which is longer than the specified 8000\n",
      "Created a chunk of size 9698, which is longer than the specified 8000\n",
      "Created a chunk of size 15270, which is longer than the specified 8000\n",
      "Created a chunk of size 8758, which is longer than the specified 8000\n",
      "Created a chunk of size 11226, which is longer than the specified 8000\n",
      "Created a chunk of size 8639, which is longer than the specified 8000\n",
      "Created a chunk of size 10245, which is longer than the specified 8000\n",
      "Created a chunk of size 11622, which is longer than the specified 8000\n",
      "Created a chunk of size 12239, which is longer than the specified 8000\n",
      "Created a chunk of size 13803, which is longer than the specified 8000\n",
      "Created a chunk of size 13682, which is longer than the specified 8000\n",
      "Created a chunk of size 11496, which is longer than the specified 8000\n",
      "Created a chunk of size 10869, which is longer than the specified 8000\n",
      "Created a chunk of size 8642, which is longer than the specified 8000\n",
      "Created a chunk of size 13449, which is longer than the specified 8000\n",
      "Created a chunk of size 13033, which is longer than the specified 8000\n",
      "Created a chunk of size 12407, which is longer than the specified 8000\n",
      "Created a chunk of size 22557, which is longer than the specified 8000\n",
      "Created a chunk of size 28778, which is longer than the specified 8000\n",
      "Created a chunk of size 18002, which is longer than the specified 8000\n",
      "Created a chunk of size 11317, which is longer than the specified 8000\n",
      "Created a chunk of size 17515, which is longer than the specified 8000\n",
      "Created a chunk of size 18484, which is longer than the specified 8000\n",
      "Created a chunk of size 8250, which is longer than the specified 8000\n",
      "Created a chunk of size 10405, which is longer than the specified 8000\n",
      "Created a chunk of size 24529, which is longer than the specified 8000\n",
      "Created a chunk of size 17968, which is longer than the specified 8000\n",
      "Created a chunk of size 8506, which is longer than the specified 8000\n",
      "Created a chunk of size 9698, which is longer than the specified 8000\n",
      "Created a chunk of size 15270, which is longer than the specified 8000\n",
      "Created a chunk of size 8758, which is longer than the specified 8000\n",
      "Created a chunk of size 11226, which is longer than the specified 8000\n",
      "Created a chunk of size 8639, which is longer than the specified 8000\n",
      "Created a chunk of size 10245, which is longer than the specified 8000\n",
      "Created a chunk of size 11622, which is longer than the specified 8000\n",
      "Created a chunk of size 12239, which is longer than the specified 8000\n",
      "Created a chunk of size 13803, which is longer than the specified 8000\n",
      "Created a chunk of size 13682, which is longer than the specified 8000\n",
      "Created a chunk of size 11496, which is longer than the specified 8000\n",
      "Created a chunk of size 10869, which is longer than the specified 8000\n",
      "Created a chunk of size 8642, which is longer than the specified 8000\n",
      "Created a chunk of size 13449, which is longer than the specified 8000\n",
      "Created a chunk of size 13033, which is longer than the specified 8000\n",
      "Created a chunk of size 12407, which is longer than the specified 8000\n",
      "Created a chunk of size 22557, which is longer than the specified 8000\n",
      "Created a chunk of size 28778, which is longer than the specified 8000\n",
      "Created a chunk of size 18002, which is longer than the specified 8000\n",
      "Created a chunk of size 11317, which is longer than the specified 8000\n",
      "Created a chunk of size 17515, which is longer than the specified 8000\n",
      "Created a chunk of size 18484, which is longer than the specified 8000\n",
      "Created a chunk of size 8250, which is longer than the specified 8000\n",
      "Created a chunk of size 10405, which is longer than the specified 8000\n",
      "Created a chunk of size 24529, which is longer than the specified 8000\n",
      "Created a chunk of size 17968, which is longer than the specified 8000\n",
      "Created a chunk of size 8506, which is longer than the specified 8000\n",
      "Created a chunk of size 9698, which is longer than the specified 8000\n",
      "Created a chunk of size 15270, which is longer than the specified 8000\n",
      "Created a chunk of size 8758, which is longer than the specified 8000\n",
      "Created a chunk of size 11226, which is longer than the specified 8000\n",
      "Created a chunk of size 8639, which is longer than the specified 8000\n",
      "Created a chunk of size 10245, which is longer than the specified 8000\n",
      "Created a chunk of size 11622, which is longer than the specified 8000\n",
      "Created a chunk of size 12239, which is longer than the specified 8000\n",
      "Created a chunk of size 13803, which is longer than the specified 8000\n",
      "Created a chunk of size 13682, which is longer than the specified 8000\n",
      "Created a chunk of size 11496, which is longer than the specified 8000\n",
      "Created a chunk of size 10869, which is longer than the specified 8000\n",
      "Created a chunk of size 8642, which is longer than the specified 8000\n",
      "Created a chunk of size 13449, which is longer than the specified 8000\n",
      "Created a chunk of size 13033, which is longer than the specified 8000\n",
      "Created a chunk of size 12407, which is longer than the specified 8000\n",
      "Created a chunk of size 22557, which is longer than the specified 8000\n",
      "Created a chunk of size 28778, which is longer than the specified 8000\n",
      "Created a chunk of size 18002, which is longer than the specified 8000\n",
      "Created a chunk of size 11317, which is longer than the specified 8000\n",
      "Created a chunk of size 17515, which is longer than the specified 8000\n",
      "Created a chunk of size 18484, which is longer than the specified 8000\n",
      "Created a chunk of size 8250, which is longer than the specified 8000\n",
      "Created a chunk of size 10405, which is longer than the specified 8000\n",
      "Created a chunk of size 24529, which is longer than the specified 8000\n",
      "Created a chunk of size 17968, which is longer than the specified 8000\n",
      "Created a chunk of size 8506, which is longer than the specified 8000\n",
      "Created a chunk of size 9698, which is longer than the specified 8000\n",
      "Created a chunk of size 15270, which is longer than the specified 8000\n",
      "Created a chunk of size 8758, which is longer than the specified 8000\n",
      "Created a chunk of size 11226, which is longer than the specified 8000\n",
      "Created a chunk of size 8639, which is longer than the specified 8000\n",
      "Created a chunk of size 10245, which is longer than the specified 8000\n",
      "Created a chunk of size 11622, which is longer than the specified 8000\n",
      "Created a chunk of size 12239, which is longer than the specified 8000\n",
      "Created a chunk of size 13803, which is longer than the specified 8000\n",
      "Created a chunk of size 13682, which is longer than the specified 8000\n",
      "Created a chunk of size 11496, which is longer than the specified 8000\n",
      "Created a chunk of size 10869, which is longer than the specified 8000\n",
      "Created a chunk of size 8642, which is longer than the specified 8000\n",
      "Created a chunk of size 13449, which is longer than the specified 8000\n",
      "Created a chunk of size 13033, which is longer than the specified 8000\n",
      "Created a chunk of size 12407, which is longer than the specified 8000\n",
      "Created a chunk of size 22557, which is longer than the specified 8000\n",
      "Created a chunk of size 28778, which is longer than the specified 8000\n",
      "Created a chunk of size 18002, which is longer than the specified 8000\n",
      "Created a chunk of size 11317, which is longer than the specified 8000\n",
      "Created a chunk of size 17515, which is longer than the specified 8000\n",
      "Created a chunk of size 18484, which is longer than the specified 8000\n",
      "Created a chunk of size 8250, which is longer than the specified 8000\n",
      "Created a chunk of size 10405, which is longer than the specified 8000\n",
      "Created a chunk of size 24529, which is longer than the specified 8000\n",
      "Created a chunk of size 17968, which is longer than the specified 8000\n",
      "Created a chunk of size 8506, which is longer than the specified 8000\n",
      "Created a chunk of size 9698, which is longer than the specified 8000\n",
      "Created a chunk of size 15270, which is longer than the specified 8000\n",
      "Created a chunk of size 8758, which is longer than the specified 8000\n",
      "Created a chunk of size 11226, which is longer than the specified 8000\n",
      "Created a chunk of size 8639, which is longer than the specified 8000\n",
      "Created a chunk of size 10245, which is longer than the specified 8000\n",
      "Created a chunk of size 11622, which is longer than the specified 8000\n",
      "Created a chunk of size 12239, which is longer than the specified 8000\n",
      "Created a chunk of size 13803, which is longer than the specified 8000\n",
      "Created a chunk of size 13682, which is longer than the specified 8000\n",
      "Created a chunk of size 11496, which is longer than the specified 8000\n",
      "Created a chunk of size 10869, which is longer than the specified 8000\n",
      "Created a chunk of size 8642, which is longer than the specified 8000\n",
      "Created a chunk of size 13449, which is longer than the specified 8000\n",
      "Created a chunk of size 13033, which is longer than the specified 8000\n",
      "Created a chunk of size 12407, which is longer than the specified 8000\n",
      "Created a chunk of size 22557, which is longer than the specified 8000\n",
      "Created a chunk of size 28778, which is longer than the specified 8000\n",
      "Created a chunk of size 18002, which is longer than the specified 8000\n",
      "Created a chunk of size 11317, which is longer than the specified 8000\n",
      "Created a chunk of size 17515, which is longer than the specified 8000\n",
      "Created a chunk of size 18484, which is longer than the specified 8000\n",
      "Created a chunk of size 8250, which is longer than the specified 8000\n",
      "Created a chunk of size 10405, which is longer than the specified 8000\n",
      "Created a chunk of size 24529, which is longer than the specified 8000\n",
      "Created a chunk of size 17968, which is longer than the specified 8000\n",
      "Created a chunk of size 8506, which is longer than the specified 8000\n",
      "Created a chunk of size 9698, which is longer than the specified 8000\n",
      "Created a chunk of size 15270, which is longer than the specified 8000\n",
      "Created a chunk of size 8758, which is longer than the specified 8000\n",
      "Created a chunk of size 11226, which is longer than the specified 8000\n",
      "Created a chunk of size 8639, which is longer than the specified 8000\n",
      "Created a chunk of size 10245, which is longer than the specified 8000\n",
      "Created a chunk of size 11622, which is longer than the specified 8000\n",
      "Created a chunk of size 12239, which is longer than the specified 8000\n",
      "Created a chunk of size 13803, which is longer than the specified 8000\n",
      "Created a chunk of size 13682, which is longer than the specified 8000\n",
      "Created a chunk of size 11496, which is longer than the specified 8000\n",
      "Created a chunk of size 10869, which is longer than the specified 8000\n",
      "Created a chunk of size 8642, which is longer than the specified 8000\n",
      "Created a chunk of size 13449, which is longer than the specified 8000\n",
      "Created a chunk of size 13033, which is longer than the specified 8000\n",
      "Created a chunk of size 12407, which is longer than the specified 8000\n",
      "Created a chunk of size 22557, which is longer than the specified 8000\n",
      "Created a chunk of size 28778, which is longer than the specified 8000\n",
      "Created a chunk of size 18002, which is longer than the specified 8000\n",
      "Created a chunk of size 11317, which is longer than the specified 8000\n",
      "Created a chunk of size 17515, which is longer than the specified 8000\n",
      "Created a chunk of size 18484, which is longer than the specified 8000\n",
      "Created a chunk of size 8250, which is longer than the specified 8000\n",
      "Created a chunk of size 10405, which is longer than the specified 8000\n",
      "Created a chunk of size 24529, which is longer than the specified 8000\n",
      "Created a chunk of size 17968, which is longer than the specified 8000\n",
      "Created a chunk of size 8506, which is longer than the specified 8000\n",
      "Created a chunk of size 9698, which is longer than the specified 8000\n",
      "Created a chunk of size 15270, which is longer than the specified 8000\n",
      "Created a chunk of size 8758, which is longer than the specified 8000\n",
      "Created a chunk of size 11226, which is longer than the specified 8000\n",
      "Created a chunk of size 8639, which is longer than the specified 8000\n",
      "Created a chunk of size 10245, which is longer than the specified 8000\n",
      "Created a chunk of size 11622, which is longer than the specified 8000\n",
      "Created a chunk of size 12239, which is longer than the specified 8000\n",
      "Created a chunk of size 13803, which is longer than the specified 8000\n",
      "Created a chunk of size 13682, which is longer than the specified 8000\n",
      "Created a chunk of size 11496, which is longer than the specified 8000\n",
      "Created a chunk of size 10869, which is longer than the specified 8000\n",
      "Created a chunk of size 8642, which is longer than the specified 8000\n",
      "Created a chunk of size 13449, which is longer than the specified 8000\n",
      "Created a chunk of size 13033, which is longer than the specified 8000\n",
      "Created a chunk of size 12407, which is longer than the specified 8000\n",
      "Created a chunk of size 22557, which is longer than the specified 8000\n",
      "Created a chunk of size 28778, which is longer than the specified 8000\n",
      "Created a chunk of size 18002, which is longer than the specified 8000\n",
      "Created a chunk of size 11317, which is longer than the specified 8000\n",
      "Created a chunk of size 17515, which is longer than the specified 8000\n",
      "Created a chunk of size 18484, which is longer than the specified 8000\n",
      "Created a chunk of size 8250, which is longer than the specified 8000\n",
      "Created a chunk of size 10405, which is longer than the specified 8000\n",
      "Created a chunk of size 24529, which is longer than the specified 8000\n",
      "Created a chunk of size 17968, which is longer than the specified 8000\n",
      "Created a chunk of size 8506, which is longer than the specified 8000\n",
      "Created a chunk of size 9698, which is longer than the specified 8000\n",
      "Created a chunk of size 15270, which is longer than the specified 8000\n",
      "Created a chunk of size 8758, which is longer than the specified 8000\n",
      "Created a chunk of size 11226, which is longer than the specified 8000\n",
      "Created a chunk of size 8639, which is longer than the specified 8000\n",
      "Created a chunk of size 10245, which is longer than the specified 8000\n",
      "Created a chunk of size 11622, which is longer than the specified 8000\n",
      "Created a chunk of size 12239, which is longer than the specified 8000\n",
      "Created a chunk of size 13803, which is longer than the specified 8000\n",
      "Created a chunk of size 13682, which is longer than the specified 8000\n",
      "Created a chunk of size 11496, which is longer than the specified 8000\n",
      "Created a chunk of size 10869, which is longer than the specified 8000\n",
      "Created a chunk of size 8642, which is longer than the specified 8000\n",
      "Created a chunk of size 13449, which is longer than the specified 8000\n",
      "Created a chunk of size 13033, which is longer than the specified 8000\n",
      "Created a chunk of size 12407, which is longer than the specified 8000\n",
      "Created a chunk of size 22557, which is longer than the specified 8000\n",
      "Created a chunk of size 28778, which is longer than the specified 8000\n",
      "Created a chunk of size 18002, which is longer than the specified 8000\n",
      "Created a chunk of size 11317, which is longer than the specified 8000\n",
      "Created a chunk of size 17515, which is longer than the specified 8000\n",
      "Created a chunk of size 18484, which is longer than the specified 8000\n",
      "Created a chunk of size 8250, which is longer than the specified 8000\n",
      "Created a chunk of size 10405, which is longer than the specified 8000\n",
      "Created a chunk of size 24529, which is longer than the specified 8000\n",
      "Created a chunk of size 17968, which is longer than the specified 8000\n",
      "Created a chunk of size 8506, which is longer than the specified 8000\n",
      "Created a chunk of size 9698, which is longer than the specified 8000\n",
      "Created a chunk of size 15270, which is longer than the specified 8000\n",
      "Created a chunk of size 8758, which is longer than the specified 8000\n",
      "Created a chunk of size 11226, which is longer than the specified 8000\n",
      "Created a chunk of size 8639, which is longer than the specified 8000\n",
      "Created a chunk of size 10245, which is longer than the specified 8000\n",
      "Created a chunk of size 11622, which is longer than the specified 8000\n",
      "Created a chunk of size 12239, which is longer than the specified 8000\n",
      "Created a chunk of size 13803, which is longer than the specified 8000\n",
      "Created a chunk of size 13682, which is longer than the specified 8000\n",
      "Created a chunk of size 11496, which is longer than the specified 8000\n",
      "Created a chunk of size 10869, which is longer than the specified 8000\n",
      "Created a chunk of size 8642, which is longer than the specified 8000\n",
      "Created a chunk of size 13449, which is longer than the specified 8000\n",
      "Created a chunk of size 13033, which is longer than the specified 8000\n",
      "Created a chunk of size 12407, which is longer than the specified 8000\n",
      "Created a chunk of size 22557, which is longer than the specified 8000\n",
      "Created a chunk of size 28778, which is longer than the specified 8000\n",
      "Created a chunk of size 18002, which is longer than the specified 8000\n",
      "Created a chunk of size 11317, which is longer than the specified 8000\n",
      "Created a chunk of size 17515, which is longer than the specified 8000\n",
      "Created a chunk of size 18484, which is longer than the specified 8000\n",
      "Created a chunk of size 8250, which is longer than the specified 8000\n",
      "Created a chunk of size 10405, which is longer than the specified 8000\n",
      "Created a chunk of size 24529, which is longer than the specified 8000\n",
      "Created a chunk of size 17968, which is longer than the specified 8000\n",
      "Created a chunk of size 8506, which is longer than the specified 8000\n",
      "C:\\Users\\FahRe\\AppData\\Local\\Temp\\ipykernel_45464\\2629379980.py:58: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "c:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.excel import UnstructuredExcelLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "file_path = r\"C:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\backend\\data\\Interaktionen nach IA-Nummern.xlsx\"\n",
    "file_path2 = r\"C:\\Users\\FahRe\\Desktop\\agentic-LLM-app\\backend\\data\\ATC-Code sortierte Textbausteine aktuell.xlsx\"\n",
    "loader1 = UnstructuredExcelLoader(file_path=file_path, sheet_name=\"Tabelle1\")\n",
    "loader2 = UnstructuredExcelLoader(file_path=file_path, sheet_name=\"Tabelle2\")\n",
    "loader3 = UnstructuredExcelLoader(file_path=file_path2, sheet_name=\"A\")\n",
    "loader4 = UnstructuredExcelLoader(file_path=file_path2, sheet_name=\"B\")\n",
    "loader5 = UnstructuredExcelLoader(file_path=file_path2, sheet_name=\"C\")\n",
    "loader6 = UnstructuredExcelLoader(file_path=file_path2, sheet_name=\"D\")\n",
    "loader7 = UnstructuredExcelLoader(file_path=file_path2, sheet_name=\"G\")\n",
    "loader8 = UnstructuredExcelLoader(file_path=file_path2, sheet_name=\"H\")\n",
    "loader9 = UnstructuredExcelLoader(file_path=file_path2, sheet_name=\"J\")\n",
    "loader10 = UnstructuredExcelLoader(file_path=file_path2, sheet_name=\"L\")\n",
    "loader11 = UnstructuredExcelLoader(file_path=file_path2, sheet_name=\"M\")\n",
    "loader12 = UnstructuredExcelLoader(file_path=file_path2, sheet_name=\"N\")\n",
    "loader13 = UnstructuredExcelLoader(file_path=file_path2, sheet_name=\"P\")\n",
    "loader14 = UnstructuredExcelLoader(file_path=file_path2, sheet_name=\"R\")\n",
    "loader15 = UnstructuredExcelLoader(file_path=file_path2, sheet_name=\"S\")\n",
    "loader16 = UnstructuredExcelLoader(file_path=file_path2, sheet_name=\"V\")\n",
    "\n",
    "\n",
    "docs_sheet1 = loader1.load()\n",
    "docs_sheet2 = loader2.load()\n",
    "docs_sheet3 = loader3.load()\n",
    "docs_sheet4 = loader4.load()\n",
    "docs_sheet5 = loader5.load()\n",
    "docs_sheet6 = loader6.load()\n",
    "docs_sheet7 = loader7.load()\n",
    "docs_sheet8 = loader8.load()\n",
    "docs_sheet9 = loader9.load()\n",
    "docs_sheet10 = loader10.load()\n",
    "docs_sheet11 = loader11.load()\n",
    "docs_sheet12 = loader12.load()\n",
    "docs_sheet13 = loader13.load()\n",
    "docs_sheet14 = loader14.load()\n",
    "docs_sheet15 = loader15.load()\n",
    "docs_sheet16 = loader16.load()\n",
    "\n",
    "\n",
    "docs = docs_sheet1 + docs_sheet2 + docs_sheet3  + docs_sheet4  + docs_sheet5  + docs_sheet6 + docs_sheet7 + docs_sheet8 + docs_sheet9 + docs_sheet10  + docs_sheet11 + docs_sheet12 + docs_sheet13 + docs_sheet14 + docs_sheet15 + docs_sheet16\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=8000,\n",
    "    chunk_overlap=1000,\n",
    "    length_function=len\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "#embedding_function = OpenAIEmbeddings(api_key=api_key, model=\"text-embedding-ada-002\")\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db = FAISS.from_documents(split_docs, embedding_function)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac386ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FahRe\\AppData\\Local\\Temp\\ipykernel_45464\\3738164229.py:9: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In den bereitgestellten Informationen ist keine Angabe zur Einnahme von Vita Hepa enthalten. Daher kann ich Ihnen leider keine genaue Auskunft zur Einnahme von Vita Hepa geben. Bitte konsultieren Sie die Packungsbeilage des Medikaments oder wenden Sie sich an Ihren Arzt oder Apotheker für genaue Anweisungen zur Einnahme.\n"
     ]
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-4.1-mini\", max_tokens=1000),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,)\n",
    "\n",
    "\n",
    "query = \"Wie sollte ich Vita Hepa einnehmen?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64098bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
