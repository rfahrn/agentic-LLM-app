# pinecone_tool.py

import os
from dotenv import load_dotenv
from openai import OpenAI
from pinecone.grpc import PineconeGRPC as Pinecone
from pinecone import ServerlessSpec
import streamlit as st
import base64
from io import BytesIO
# Load environment variables (for API keys)
load_dotenv()

# Configuration
OPENAI_API_KEY = st.secrets.OPENAI.OPENAI_API_KEY
PINECONE_API_KEY = st.secrets.PINECONE.PINECONE_API_KEY
PINECONE_INDEX = "medguides-index"
EMBEDDING_MODEL = "text-embedding-3-large"
GPT_MODEL = "gpt-4o-mini"

# Initialize clients
openai_client = OpenAI(api_key=OPENAI_API_KEY)
pinecone = Pinecone(api_key=PINECONE_API_KEY)
index = pinecone.Index(PINECONE_INDEX)


# â”€â”€â”€ Embed Query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def embed_query(query: str):
    res = openai_client.embeddings.create(model=EMBEDDING_MODEL, input=query)
    return res.data[0].embedding


# â”€â”€â”€ Convert PDF Page to Base64 Image â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from pdf2image import convert_from_path
def get_pdf_page_as_base64_image(filename: str, page_num: int):
    try:
        pdf_path = os.path.join(r"C:\Users\FahRe\Desktop\agentic-LLM-app\backend\data\MedicationGuides_2025_05_19", filename)
        images = convert_from_path(pdf_path, first_page=int(page_num), last_page=int(page_num))
        buffered = BytesIO()
        images[0].save(buffered, format="PNG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
        return f"<img src='data:image/png;base64,{img_base64}' width='700'/>"
    except Exception as e:
        return f"âš ï¸ Fehler beim Generieren der Seitenvorschau ({filename}, S. {page_num}): {e}"

# â”€â”€â”€ Context Retrieval â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def retrieve_context(query: str, top_k=8):
    xq = embed_query(query)
    response = index.query(vector=xq, top_k=top_k, include_metadata=True)

    context_blocks = []
    sources = []
    scores = []

    for match in response['matches']:
        meta = match.get("metadata", {})
        text = meta.get("source_text", "").strip()
        if not text:
            continue

        filename = meta.get("filename", "unknown.pdf")
        page = meta.get("page", "?")
        score = round(match.get("score", 0), 2)
        scores.append(score)
        short_filename = filename.split('_TABLET_')[0].replace('_', ' ').title()
        context_blocks.append(f"From **{short_filename}**, page **{page}**:\n{text}")
        sources.append({
            "filename": filename,
            "page": page,
            "score": score,
            "preview": text[:250] + ("..." if len(text) > 250 else "")
        })
    sources = sorted(sources, key=lambda x: x['score'], reverse=True)[:5]

    context = "\n\n---\n\n".join(context_blocks)
    avg_score = round(sum(scores) / len(scores), 2) if scores else 0

    return context, sources, avg_score


# â”€â”€â”€ RAG Workflow with GPT + Markdown Answer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_medguides_with_rag(query: str, top_k=8) -> str:
    context, sources, avg_score = retrieve_context(query, top_k=top_k)

    if not context:
        return "âš ï¸ Keine passenden Informationen im lokalen PDF-Vektorindex gefunden."

    prompt = f"""Du bist ein pharmazeutischer Assistent. Beantworte die folgende Frage **ausschlieÃŸlich basierend auf dem untenstehenden Kontext**.
            Antworte in **Markdown**, gegliedert in eine **Zusammenfassung**, eine **Detailanalyse** (Text oder Tabelle), und schlieÃŸe mit **Quellenangaben** ab. Wenn Tabellen im Kontext enthalten sind, stelle sie als Markdown-Tabelle dar. Zitiere die Quelle jeder Information als (Quelle: DATEI.pdf, S. X).

            ### Kontext:
            {context}

            ### Frage:
            {query}

            ### Antwort:"""

    res = openai_client.chat.completions.create(
        model=GPT_MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    answer = res.choices[0].message.content.strip()

    citation_block = "\n".join([
        f"- **{s['filename']}**, Seite {s['page']} â€“ Relevanz-Score: `{s['score']}`"
        for s in sources
    ])

    final_md = f"""{answer}

---

### ğŸ“Š Quellen & Relevanzbewertung
**Durchschnittlicher Ãœbereinstimmungswert:** `{avg_score}`  
{citation_block}
"""
    return final_md, sources